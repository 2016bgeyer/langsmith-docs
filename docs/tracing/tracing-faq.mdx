---
sidebar_label: FAQs
sidebar_position: 2
toc_max_heading_level: 3
---

import {
  CodeTabs,
  PythonBlock,
  TypeScriptBlock,
} from '@site/src/components/InstructionsWithCode';

import {
  RunTreeQuickStartCodeTabs,
  TraceableQuickStartCodeBlock,
  TraceableThreadingCodeBlock,
} from '@site/src/components/QuickStart';

# Tracing FAQs

The following are some frequently asked questions about logging runs to LangSmith:

### How do I group traces into a project?

All runs are logged to a project. If left unspecified, the tracer project is set to `default`.
You can set the `LANGCHAIN_PROJECT` environment variable to configure a custom project name for an entire application
run. This is best done before executing your program.

```bash
export LANGCHAIN_PROJECT="My Project"
```

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.chat_models import ChatAnthropic
llm = ChatAnthropic()\n
llm.predict("Hello, World!")`),
    TypeScriptBlock(`import { ChatAnthropic } from "langchain/chat_models/anthropic";\n
const llm = new ChatAnthropic();
await llm.predict("Hello, World!");`),
  ]}
  groupId="client-language"
/>

### How do I change the tracer project?

When global environment variables are too broad, you can also set the project name for a specific tracer instance:

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.callbacks.tracers import LangChainTracer\n
tracer = LangChainTracer(project_name="My Project")
# Use the tracer as one of the callbacks to a chain or LLM run
agent.run("How many people live in canada as of 2023?", callbacks=[tracer])
`),
    TypeScriptBlock(`import { LangChainTracer } from "langchain/callbacks/tracers";\n
const tracer = new LangChainTracer({ projectName: "My Project" });
// Use the tracer as one of the callbacks to a chain or LLM run
await agent.call({ input: "How many people live in canada as of 2023?" }, [
  tracer,
]);`),
  ]}
  groupId="client-language"
/>

The Python tracer also has a context manager you can use to set the project for anything within the context:

```python
from langchain.callbacks import tracing_v2_enabled

with tracing_v2_enabled(project_name="My Project"):
    agent.run("How many people live in canada as of 2023?")
```

### How do I add tags to runs?

The `LangChainTracer` supports adding tags to any run trace. These tags are visible in the UI and through the API and can be used to filter and analyze runs. You can tag a single object's runs by adding them when the object is initialized, or you can tag all nested runs by passing them to the object's function call. The following code snippet shows how to do both, assuming you've set the appropriate environment variables to log traces.

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate\n
llm = ChatOpenAI(temperature=0, tags=["my-llm-tag"])
prompt = PromptTemplate.from_template("Say {input}")
chain = LLMChain(llm=llm, prompt=prompt, tags=["my-bash-tag", "another-tag"])\n
chain("Hello, World!", tags=["shared-tags"])
`),
    TypeScriptBlock(`import { LLMChain, PromptTemplate } from "langchain";
import { ChatOpenAI } from "langchain/chat_models/openai";\n
const llm = new ChatOpenAI({ temperature: 0, tags: ["my-llm-tag"] });
const prompt = PromptTemplate.fromTemplate("Say {input}");
const chain = new LLMChain({
  llm,
  prompt,
  tags: ["my-bash-tag", "another-tag"],
});\n
await chain.call({ input: "Hello, World!", tags: ["shared-tags"] });
`),
  ]}
  groupId="client-language"
/>

### How do I add metadata to runs?

Similar to tags, LangSmith permits associating arbitrary key-value pairs as metadata within runs. Below is an example:

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain\n
chat_model = ChatOpenAI()
chain = LLMChain.from_string(llm=chat_model, template="What's the answer to {input}?")\n
chain({"input": "What is the meaning of life?"}, metadata={"my_key": "My Value"})`),
    TypeScriptBlock(`import { ChatOpenAI } from "langchain/chat_models/openai";
import { LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";\n
const chatModel = new ChatOpenAI();
const chain = new LLMChain({
llm: chatModel,
prompt: PromptTemplate.fromTemplate("What's the answer to {input}?"),
});\n
await chain.call(
  { input: "What is the meaning of life?" },
  { metadata: { myKey: "My Value" } }
);
`),
]}
groupId="client-language"
/>

### How do I use LangSmith in my AB Testing framework?

AB tests are a great way to compare the performance of different models or chains. Currently, this is best facilitated using [metadata](#how-do-i-add-metadata-to-runs) (though [tags](#how-do-i-add-tags-to-runs) also suffice).
Inject the experiment ID or testing variant(s) in as metadata values, then use the SDK or web app to compare metrics over subsets.

<CodeTabs
  tabs={[
    PythonBlock(`ffrom langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain\n
chat_model = ChatOpenAI()
chain = LLMChain.from_string(llm=chat_model, template="What's the answer to {input}?")\n
chain({"input": "What is the meaning of life?"}, metadata={"variant": "abc123"})`),
    TypeScriptBlock(`import { ChatOpenAI } from "langchain/chat_models/openai";
import { LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";\n
const chatModel = new ChatOpenAI();
const chain = new LLMChain({
  llm: chatModel,
  prompt: PromptTemplate.fromTemplate("What's the answer to {input}?"),
});\n
await chain.call(
  { input: "What is the meaning of life?" },
  { metadata: { variant: "abc123" } }
);
`),
  ]}
  groupId="client-language"
/>

Then in the runs page of the web app, you can filter runs using the query `has(metadata, '{"variant": "abc123"}')`. Similarly using the LangSmith SDK:

<CodeTabs
  tabs={[
    PythonBlock(`from langsmith import Client\n
client = Client()
runs = list(client.list_runs(
    filter='has(metadata, \\'{"variant": "abc123"}\\')',
))`),
    TypeScriptBlock(`import { Client, Run } from "langsmith";\n
const client = new Client();
for await (const run of client.listRuns({
  filter: 'has(metadata, \\'{"variant": "abc123"}\\')',
})) {
  runs.push(run);
}
`),
  ]}
  groupId="client-language"
/>

### How do I group runs from multi-turn interactions?

Many application experiences involve multiple interactions outside a single call to an agent or chain.
By default, these are saved as separate runs (though the memory and chat history is logged where appropriate).

To group these runs together, we offer convenience methods in both Python and JS that save all runs
within a defined context beneath a virtual parent run.

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.callbacks.manager import (
    trace_as_chain_group, 
    atrace_as_chain_group,
)\n
with trace_as_chain_group("my_group_name") as group_manager:
    """Pass the group_manager as a callback to group all runs
    within this context"""\n
# Or for async code
async with atrace_as_chain_group("my_group_name") as async_group_manager:
    """Async applications are better suited with the async callback manager"""\n
# Example usage:
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate\n
llm = ChatOpenAI(temperature=0.9)
prompt = PromptTemplate(
    input_variables=["question"],
    template="What is the answer to {question}?",
)
chain = LLMChain(llm=llm, prompt=prompt)
with trace_as_chain_group("my_group") as group_manager:
    chain.run(question="What is your name?", callbacks=group_manager)
    chain.run(question="What is your quest?", callbacks=group_manager)
    chain.run(question="What is your favorite color?", callbacks=group_manager)`),
    TypeScriptBlock(`import { CallbackManager, traceAsGroup, TraceGroup } from "langchain/callbacks";
import { LLMChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { PromptTemplate } from "langchain/prompts";\n
// Initialize the LLMChain
const llm = new ChatOpenAI({ temperature: 0.9 });
const prompt = new PromptTemplate({
  inputVariables: ["question"],
  template: "What is the answer to {question}?",
});
const chain = new LLMChain({ llm, prompt });\n
// You can group runs together using the traceAsGroup function
const blockResult = await traceAsGroup(
  { name: "my_group_name" },
  async (manager: CallbackManager, questions: string[]) => {
    await chain.call({ question: questions[0] }, manager);
    await chain.call({ question: questions[1] }, manager);
    const finalResult = await chain.call({ question: questions[2] }, manager);
    return finalResult;
  },
  [
    "What is your name?",
    "What is your quest?",
    "What is your favorite color?",
  ]
);\n
// Or you can manually control the start and end of the grouped run
const traceGroup = new TraceGroup("my_group_name");
const groupManager = await traceGroup.start();
try {
  await chain.call({ question: "What is your name?" }, groupManager);
  await chain.call({ question: "What is your quest?" }, groupManager);
  await chain.call(
    { question: "What is the airspeed velocity of an unladen swallow?" },
    groupManager
  );
} finally {
  // Code goes here
  await traceGroup.end();
}
`),
  ]}
  groupId="client-language"
/>

### How do I get the run ID from a call?

The run ID is returned in the call response under the `__run` key. In python, it is not returned by default. There you will have to pass the `include_run_id=True` parameter to the call function. Example:

<CodeTabs
  tabs={[
    PythonBlock(`import os
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain\n
chain = LLMChain.from_string(ChatOpenAI(), "Say hi to {name}")
def main():
  response = chain("Clara", include_run_info=True)
  run_id = response["__run"].run_id
  print(run_id)
`),
    TypeScriptBlock(`import { ChatOpenAI } from "langchain/chat_models/openai";
import { LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";\n
const prompt = PromptTemplate.fromTemplate("Say hi to {name}");
const chain = new LLMChain({
  llm: new ChatOpenAI(),
  prompt: prompt,
});\n
async function main() {
  const response = await chain.call({ name: "Clara" });
  console.log(response.__run);
}\n
main();
`),
  ]}
  groupId="client-language"
/>

### How do I get the URL of the run?

Runs are streamed to whichever project you have configured ("default" if none is set), and you can view them by opening the project page. To programmatically access the run's URL, you can
use the LangSmith client. This can be conveniently used in conjunction with [the above method to get the run ID from a call](#how-do-i-get-the-run-id-from-a-call). Below is an example.

<i>Note: This requires</i> <code>langsmith>=0.0.11</code>

<CodeTabs
  tabs={[
    PythonBlock(`from langsmith import Client\n
client = Client()
run = client.read_run("<run_id>")
print(run.url)`),
    TypeScriptBlock(`import { Client } from "langsmith";\n
const client = new Client();
const runUrl = await client.getRunUrl({runId: "<run_id>"});
console.log(runUrl);
`),
  ]}
  groupId="client-language"
/>

### How do I log traces without environment variables?

Some situations don't permit the use of environment variables or don't expose `process.env`. This is mostly pertinent when running LangChain apps in certain JavaScript runtime environments. To add tracing in these situations, you can manually create the `LangChainTracer` callback and pass it to the chain, LLM, or other LangChain component, either when initializing or in the call itself. This is the same tactic used for [changing the tracer project](#how-do-i-change-the-tracer-project) within a program.

Example:

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.callbacks import LangChainTracer
from langchain.chat_models import ChatOpenAI
from langsmith import Client\n
callbacks = [
  LangChainTracer(
    project_name="YOUR_PROJECT_NAME_HERE",
    client=Client(
      api_url="https://api.smith.langchain.com",
      api_key="YOUR_API_KEY_HERE"
    )
  )
]\n
llm = ChatOpenAI(callbacks=callbacks)
llm.predict("Hello, world!")
`),
    TypeScriptBlock(`import { Client } from "langsmith";
import { LangChainTracer } from "langchain/callbacks";
import { ChatOpenAI } from "langchain/chat_models/openai";\n
const llm = new ChatOpenAI({
  callbacks: [
    new LangChainTracer({
      projectName: "YOUR_PROJECT_NAME_HERE",
      client: new Client({
        apiUrl: "https://api.smith.langchain.com",
        apiKey: "YOUR_API_KEY_HERE",
      }),
    }),
  ],
});
await llm.predict("Hello, world!");
`),
  ]}
  groupId="client-language"
/>

This tactic is also useful for when you have multiple chains running in a shared environment but want to log their run traces to different projects.

### How do I export runs?

We are working to add better data connectors and export options. For now, you can use the LangSmith API directly to export runs. Below are a couple examples:

<CodeTabs
  tabs={[
    PythonBlock(`from langsmith import Client\n
client = Client()
# Download all runs in a project
project_runs = client.list_runs(project_name="<your_project>")\n
# List only LLM runs in the last 24 hours
todays_runs = client.list_runs(
    start_time=datetime.now() - timedelta(days=1),
    run_type="llm",
)\n
# You can build complex filters and queries if needed.
# Filter by runs that have a feedback key of "Correctness==1.0"
# living in the specified project.
correct_runs = client.list_runs(
    project_name="<your_project>",
    # More complex filters can be specified
    filter='and(eq(feedback_key, "Correctness"), eq(feedback_score, 1.0))',
)
`),
    TypeScriptBlock(`import { Client, Run } from "langsmith";\n
const client = new Client();\n
// Download runs in a project
const todaysRuns: Run[] = [];
for await (const run of client.listRuns({
  projectName: "<your_project>",
  startTime: new Date(Date.now() - 1000 * 60 * 60 * 24),
  runType: "llm",
})) {
  todaysRuns.push(run);
}\n
// You can use a number of filters for feedback, tags, and other fields
// filter by runs that have a feedback key of "Correctness==1.0"
const correctRuns: Run[] = [];
for await (const run of  client.listRuns({
  projectName: "<your_project>",
  // More complex filters can be specified
  filter: 'and(eq(feedback_key, "Correctness"), eq(feedback_score, 1.0))',
})) {
  correctRuns.push(run);
}
`),
  ]}
  groupId="client-language"
/>

Check out the [exporting runs](use-cases/export-runs) directory for more examples of how to export runs, or the [local run filtering documentation](use-cases/export-runs/local) on how to construct more complex filters.

### How do I ensure logging is completed before exiting my application?

In LangChain, LangSmith's tracing is asynchronous (or done in a background thread in python) to avoid obstructing your production application. This means that if you're process may end before all traces are successfully posted to LangSmith. This is especially prevelant in a serverless environment, where your VM may be terminated immediately once your chain or agent completes.

LangChain exposes methods to wait for traces to be submitted before exiting your application.

Below is an example:

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.chat_models import ChatOpenAI
from langchain.callbacks.tracers.langchain import wait_for_all_tracers\n
llm = ChatOpenAI()
try:
    llm.predict("Hello, World!")
finally:
    # highlight-next-line
    wait_for_all_tracers()
`),
    TypeScriptBlock(`import { ChatOpenAI } from "langchain/chat_models/openai";
import { awaitAllCallbacks } from "langchain/callbacks";\n
const llm = new ChatOpenAI();
llm.predict("Hello, World!").finally(() => awaitAllCallbacks());
`),
  ]}
  groupId="client-language"
/>


### How do I log to LangSmith if I am not using LangChain?

The most reliable way to log run trees to LangSmith is by using the `RunTree` object. Below is an example:

<RunTreeQuickStartCodeTabs />

In python, we also provide an experimental `@traceable` decorator that automatically handles the the life cycle of the `RunTree` object. Below is an example that will log a nested run tree to LangSmith:

<TraceableQuickStartCodeBlock />

`traceable` functions work out of the box for synchronous and async calls. If you want to call sub-chains/functions on separate threads, you will have to manually pass the run tree to the child functions. To do so, you should update the parent function to accept a `run_tree` argument (which is injected by the decorator) and pass it to the child functions through the `langsmith_extra` keyword argument. Below is an example:

<TraceableThreadingCodeBlock />

### When logging with the SDK, which fields can I update when I patch?

The following fields can be updated when patching a run:

- end_time: `datetime.datetime`
- error: `str | None`
- outputs: `Dict | None`
- events: `list[dict] | None`

Once an `end_time` is set on a run, it is marked as "closed" and can no longer be updated. This is the case if you include an end time in the initial run creation `post` request or if you do so in a later `patch` request.

### How do I search and filter runs?

You can flexibly search and filter for runs directly in the web app or via the SDK. See [exporting runs locally guide](use-cases/export-runs/local) for more examples.

In the web app, you can search through 'traces' (the top level runs) or through all runs using the search bar in the runs table. This uses the [run filtering](use-cases/export-runs/local#run-filtering) query syntax.


### How do I use the playground for runs logged using the SDK?

The LangSmith playground doesn't yet support re-running arbitrary runs logged using the SDK. We are working to extend logging and playground support for users not using LangChain components in their app.