---
sidebar_label: FAQs
sidebar_position: 2
toc_max_heading_level: 3
---

import {
  CodeTabs,
  PythonBlock,
  TypeScriptBlock,
} from "@site/src/components/InstructionsWithCode";

import {
  RunTreeQuickStartCodeTabs,
  TraceableQuickStartCodeBlock,
  TraceableThreadingCodeBlock,
} from "@site/src/components/QuickStart";

import {
  AccessRunIdBlock,
} from "@site/src/components/TracingFaq";

# Tracing FAQs

The following are some frequently asked questions about logging runs to LangSmith:

### How do I group traces into a project?

All runs are logged to a project. If left unspecified, the tracer project is set to `default`.
You can set the `LANGCHAIN_PROJECT` environment variable to configure a custom project name for an entire application
run. This is best done before executing your program.

```bash
export LANGCHAIN_PROJECT="My Project"
```

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.chat_models import ChatAnthropic
llm = ChatAnthropic()\n
llm.invoke("Hello, World!")`),
    TypeScriptBlock(`import { ChatAnthropic } from "langchain/chat_models/anthropic";\n
const llm = new ChatAnthropic();
await llm.invoke("Hello, World!");`),
  ]}
  groupId="client-language"
/>

### How do I change the tracer project?

When global environment variables are too broad, you can also set the project name for a specific tracer instance:

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.callbacks.tracers import LangChainTracer\n
tracer = LangChainTracer(project_name="My Project")
chain.invoke({"query": "How many people live in canada as of 2023?"}, config={"callbacks": [tracer]})
`),
    TypeScriptBlock(`import { LangChainTracer } from "langchain/callbacks";\n
const tracer = new LangChainTracer({ projectName: "My Project" });
await chain.invoke(
  {
    query: "How many people live in canada as of 2023?"
  },
  { callbacks: [tracer] }
);
`),
  ]}
  groupId="client-language"
/>

The Python tracer also has a context manager you can use to set the project for anything within the context:

```python
from langchain.callbacks import tracing_v2_enabled

with tracing_v2_enabled(project_name="My Project"):
    chain.invoke({"query": "How many people live in canada as of 2023?"})
```

### How do I add tags to runs?

The `LangChainTracer` supports adding tags to any run trace. These tags are visible in the UI and through the API and can be used to filter and analyze runs. You can tag a single object's runs by adding them when the object is initialized, or you can tag all nested runs by passing them to the object's function call. The following code snippet shows how to do both, assuming you've set the appropriate environment variables to log traces.

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate\n
llm = ChatOpenAI(temperature=0, tags=["my-llm-tag"])
prompt = PromptTemplate.from_template("Say {input}")
chain = LLMChain(llm=llm, prompt=prompt, tags=["my-bash-tag", "another-tag"])\n
chain.invoke("Hello, World!", {"tags": ["shared-tags"]})
`),
    TypeScriptBlock(`import { LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";
import { ChatOpenAI } from "langchain/chat_models/openai";\n
const llm = new ChatOpenAI({ temperature: 0, tags: ["my-llm-tag"] });
const prompt = PromptTemplate.fromTemplate("Say {input}");
const chain = new LLMChain({
  llm,
  prompt,
  tags: ["my-bash-tag", "another-tag"],
});\n
await chain.invoke({ input: "Hello, World!", tags: ["shared-tags"] });
`),
  ]}
  groupId="client-language"
/>

### How do I add metadata to runs?

Similar to tags, LangSmith permits associating arbitrary key-value pairs as metadata within runs. Below is an example:

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain\n
chat_model = ChatOpenAI()
chain = LLMChain.from_string(llm=chat_model, template="What's the answer to {input}?")\n
chain.invoke({"input": "What is the meaning of life?"}, {"metadata": {"my_key": "My Value"}})`),
    TypeScriptBlock(`import { ChatOpenAI } from "langchain/chat_models/openai";
import { LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";\n
const chatModel = new ChatOpenAI();
const chain = new LLMChain({
  llm: chatModel,
  prompt: PromptTemplate.fromTemplate("What's the answer to {input}?"),
});\n
await chain.invoke(
  { input: "What is the meaning of life?" },
  { metadata: { myKey: "My Value" } }
);
`),
  ]}
  groupId="client-language"
/>

### How do I customize the name of a run?

When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').
You can customize chain and runnable trace names by calling `new_chain = chain.with_config({"run_name": "MyRunName"})`.

Example:

```python
configured_chain = chain.with_config({"run_name": "MyCustomChain"})
configured_chain.invoke({"query": "What is the meaning of life?"})
```

For more examples of this, check out the [recipe on customizing run names](https://github.com/langchain-ai/langsmith-cookbook/blob/main/tracing-examples/runnable-naming/run-naming.ipynb).

If you are logging using the LangSmith SDK or REST API directly, you can set the name of the run manually to be whatever you like.

### How do I use LangSmith in my AB Testing framework?

AB tests are a great way to compare the performance of different models or chains. Currently, this is best facilitated using [metadata](#how-do-i-add-metadata-to-runs) (though [tags](#how-do-i-add-tags-to-runs) also suffice).
Inject the experiment ID or testing variant(s) in as metadata values, then use the SDK or web app to compare metrics over subsets.

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain\n
chat_model = ChatOpenAI()
chain = LLMChain.from_string(llm=chat_model, template="What's the answer to {input}?")\n
chain.invoke({"input": "What is the meaning of life?"}, {"metadata": {"variant": "abc123"}})`),
    TypeScriptBlock(`import { ChatOpenAI } from "langchain/chat_models/openai";
import { LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";\n
const chatModel = new ChatOpenAI();
const chain = new LLMChain({
  llm: chatModel,
  prompt: PromptTemplate.fromTemplate("What's the answer to {input}?"),
});\n
await chain.invoke(
  { input: "What is the meaning of life?" },
  { metadata: { variant: "abc123" } }
);
`),
  ]}
  groupId="client-language"
/>

Then in the runs page of the web app, you can filter runs using the query `has(metadata, '{"variant": "abc123"}')`. Similarly using the LangSmith SDK:

<CodeTabs
  tabs={[
    PythonBlock(`from langsmith import Client\n
client = Client()
runs = list(client.list_runs(
    project_name="<your_project>",
    filter='has(metadata, \\'{"variant": "abc123"}\\')',
))`),
    TypeScriptBlock(`import { Client, Run } from "langsmith";\n
const client = new Client();
const runs: Run[] = [];
for await (const run of client.listRuns({
  projectName: "<your_project>",
  filter: 'has(metadata, \\'{"variant": "abc123"}\\')',
})) {
  runs.push(run);
}
`),
  ]}
  groupId="client-language"
/>

### How do I group runs from multi-turn interactions?

Many application experiences involve multiple interactions outside a single call to an agent or chain.
By default, these are saved as separate runs (though the memory and chat history is logged where appropriate).

To group these runs together, we offer convenience methods in both Python and JS that save all runs
within a defined context beneath a virtual parent run.

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.callbacks.manager import (
    trace_as_chain_group,
    atrace_as_chain_group,
)\n
with trace_as_chain_group("my_group_name") as group_manager:
    """Pass the group_manager as a callback to group all runs
    within this context"""\n
# Or for async code
async with atrace_as_chain_group("my_group_name") as async_group_manager:
    """Async applications are better suited with the async callback manager"""\n
# Example usage:
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate\n
llm = ChatOpenAI(temperature=0.9)
prompt = PromptTemplate(
    input_variables=["question"],
    template="What is the answer to {question}?",
)
chain = LLMChain(llm=llm, prompt=prompt)
with trace_as_chain_group("my_group") as group_manager:
    chain.invoke({"question": "What is your name?"}, config={"callbacks": group_manager})
    chain.invoke({"question": "What is your quest?"}, config={"callbacks": group_manager})
    chain.invoke({"question": "What is your favorite color?"}, config={"callbacks": group_manager})`),
    TypeScriptBlock(`import { CallbackManager, traceAsGroup, TraceGroup } from "langchain/callbacks";
import { LLMChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { PromptTemplate } from "langchain/prompts";\n
// Initialize the LLMChain
const llm = new ChatOpenAI({ temperature: 0.9 });
const prompt = new PromptTemplate({
  inputVariables: ["question"],
  template: "What is the answer to {question}?",
});
const chain = new LLMChain({ llm, prompt });
// You can group runs together using the traceAsGroup function
const blockResult = await traceAsGroup(
  { name: "my_group_name" },
  async (manager: CallbackManager, questions: string[]) => {
    await chain.invoke({ question: questions[0] }, { callbacks: manager });
    await chain.invoke({ question: questions[1] }, { callbacks: manager });
    const finalResult = await chain.invoke(
      { question: questions[2] },
      { callbacks: manager }
    );
    return finalResult;
  },
  ["What is your name?", "What is your quest?", "What is your favorite color?"]
);
console.log(blockResult);\n
// Or you can manually control the start and end of the grouped run
const traceGroup = new TraceGroup("my_group_name");
const groupManager = await traceGroup.start();
try {
  await chain.invoke({ question: "What is your name?" }, { callbacks: groupManager });
  await chain.invoke({ question: "What is your quest?" }, { callbacks: groupManager });
  await chain.invoke(
    { question: "What is the airspeed velocity of an unladen swallow?" },
    { callbacks: groupManager }
  );
} finally {
  // Code goes here
  await traceGroup.end();
}
`),
  ]}
  groupId="client-language"
/>

### How do I get the run ID from a call?

In Typescript, the run ID is returned in the call response under the `__run` key. In python, we recommend using the run collector callback.
Below is an example:

<AccessRunIdBlock />


### How do I get the URL of the run?

Runs are streamed to whichever project you have configured ("default" if none is set), and you can view them by opening the project page. To programmatically access the run's URL, you can
use the LangSmith client. This can be conveniently used in conjunction with [the above method to get the run ID from a call](#how-do-i-get-the-run-id-from-a-call). Below is an example.

<i>Note: This requires</i> <code>langsmith>=0.0.11</code>

<CodeTabs
  tabs={[
    PythonBlock(`from langsmith import Client\n
client = Client()
run = client.read_run("<run_id>")
print(run.url)`),
    TypeScriptBlock(`import { Client } from "langsmith";\n
const client = new Client();
const runUrl = await client.getRunUrl({runId: "<run_id>"});
console.log(runUrl);
`),
  ]}
  groupId="client-language"
/>

### How do I log traces without environment variables?

Some situations don't permit the use of environment variables or don't expose `process.env`. This is mostly pertinent when running LangChain apps in certain JavaScript runtime environments. To add tracing in these situations, you can manually create the `LangChainTracer` callback and pass it to the chain, LLM, or other LangChain component, either when initializing or in the call itself. This is the same tactic used for [changing the tracer project](#how-do-i-change-the-tracer-project) within a program.

Example:

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.callbacks import LangChainTracer
from langchain.chat_models import ChatOpenAI
from langsmith import Client\n
callbacks = [
  LangChainTracer(
    project_name="YOUR_PROJECT_NAME_HERE",
    client=Client(
      api_url="https://api.smith.langchain.com",
      api_key="YOUR_API_KEY_HERE"
    )
  )
]\n
llm = ChatOpenAI()
llm.invoke("Hello, world!", config={"callbacks": callbacks})
`),
    TypeScriptBlock(`import { Client } from "langsmith";
import { LangChainTracer } from "langchain/callbacks";
import { ChatOpenAI } from "langchain/chat_models/openai";\n
const callbacks = [
  new LangChainTracer({
    projectName: "YOUR_PROJECT_NAME_HERE",
    client: new Client({
      apiUrl: "https://api.smith.langchain.com",
      apiKey: "YOUR_API_KEY_HERE",
    }),
  }),
];
const llm = new ChatOpenAI({});
await llm.invoke("Hello, world!", { callbacks });
`),
  ]}
  groupId="client-language"
/>

This tactic is also useful for when you have multiple chains running in a shared environment but want to log their run traces to different projects.

### How do I turn tracing off?

If you've decided you no longer want to trace your runs, you can remove the environment variables configured to start tracing in the first place.
By unsetting the `LANGCHAIN_TRACING_V2` environment variable, traces will no longer be logged to LangSmith.

```bash
unset LANGCHAIN_TRACING_V2
```

Unsetting an environment variable while a program is in the middle of execution is not guaranteed to terminate logging.
If you want to selectively log runs or stop tracing mid-execution, you must not start tracing using the `LANGCHAIN_TRACING_V2` environment variable and
instead selectively log runs by passing the `LangChainTracer` callback to the LangChain object (or by using the context manager in python).

### How do I export runs?

We are working to add better data connectors and export options. For now, you can use the LangSmith API directly to export runs. Below are a couple examples:

<CodeTabs
  tabs={[
    PythonBlock(`from datetime import datetime, timedelta
from langsmith import Client\n
client = Client()
# Download all runs in a project
project_runs = client.list_runs(project_name="<your_project>")\n
# List only LLM runs in the last 24 hours
todays_runs = client.list_runs(
    start_time=datetime.now() - timedelta(days=1),
    run_type="llm",
)\n
# You can build complex filters and queries if needed.
# Filter by runs that have a feedback key of "Correctness==1.0"
# living in the specified project.
correct_runs = client.list_runs(
    project_name="<your_project>",
    # More complex filters can be specified
    filter='and(eq(feedback_key, "Correctness"), eq(feedback_score, 1.0))',
)
`),
    TypeScriptBlock(`import { Client, Run } from "langsmith";\n
const client = new Client();\n
// Download runs in a project
const todaysRuns: Run[] = [];
for await (const run of client.listRuns({
  projectName: "<your_project>",
  startTime: new Date(Date.now() - 1000 * 60 * 60 * 24),
  runType: "llm",
})) {
  todaysRuns.push(run);
}\n
// You can use a number of filters for feedback, tags, and other fields
// filter by runs that have a feedback key of "Correctness==1.0"
const correctRuns: Run[] = [];
for await (const run of  client.listRuns({
  projectName: "<your_project>",
  // More complex filters can be specified
  filter: 'and(eq(feedback_key, "Correctness"), eq(feedback_score, 1.0))',
})) {
  correctRuns.push(run);
}
`),
  ]}
  groupId="client-language"
/>

Check out the [exporting runs](use-cases/export-runs) directory for more examples of how to export runs, or the [local run filtering documentation](use-cases/export-runs/local) on how to construct more complex filters.

### How do I ensure logging is completed before exiting my application?

In LangChain.py, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that if you're process may end before all traces are successfully posted to LangSmith. This is especially prevelant in a serverless environment, where your VM may be terminated immediately once your chain or agent completes.

In LangChain.js, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the `LANGCHAIN_CALLBACKS_BACKGROUND` environment variable to `"true"`.

For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.

Below is an example:

<CodeTabs
  tabs={[
    PythonBlock(`from langchain.chat_models import ChatOpenAI
from langchain.callbacks.tracers.langchain import wait_for_all_tracers\n
llm = ChatOpenAI()
try:
    llm.invoke("Hello, World!")
finally:
    # highlight-next-line
    wait_for_all_tracers()
`),
    TypeScriptBlock(`import { ChatOpenAI } from "langchain/chat_models/openai";
import { awaitAllCallbacks } from "langchain/callbacks";\n
try {
  const llm = new ChatOpenAI();
  const response = await llm.invoke("Hello, World!");
} catch (e) {
  // handle error
} finally {
  await awaitAllCallbacks();
}
`),
  ]}
  groupId="client-language"
/>

### How do I log to LangSmith if I am not using LangChain?

The most reliable way to log run trees to LangSmith is by using the `RunTree` object. Below is an example:

<RunTreeQuickStartCodeTabs />

`traceable` functions work out of the box for synchronous and async calls. If you want to call sub-chains/functions on separate threads, you will have to manually pass the run tree to the child functions. To do so, you should update the parent function to accept a `run_tree` argument (which is injected by the decorator) and pass it to the child functions through the `langsmith_extra` keyword argument. Below is an example:

<TraceableThreadingCodeBlock />

For more information, check out the [traceable notebook](https://github.com/langchain-ai/langsmith-cookbook/blob/main/tracing-examples/traceable/tracing_without_langchain.ipynb in the LangSmith cookbook.

### When logging with the SDK, which fields can I update when I patch?

The following fields can be updated when patching a run:

- end_time: `datetime.datetime`
- error: `str | None`
- outputs: `Dict | None`
- events: `list[dict] | None`

Once an `end_time` is set on a run, it is marked as "closed" and can no longer be updated. This is the case if you include an end time in the initial run creation `post` request or if you do so in a later `patch` request.

### How do I search and filter runs?

You can flexibly search and filter for runs directly in the web app or via the SDK. See [exporting runs locally guide](use-cases/export-runs/local) for more examples.

In the web app, you can search through 'traces' (the top level runs) or through all runs using the search bar in the runs table. This uses the [run filtering](use-cases/export-runs/local#run-filtering) query syntax.

### How do I use the playground for runs logged using the SDK?

The LangSmith playground doesn't yet support re-running arbitrary runs logged using the SDK. We are working to extend logging and playground support for users not using LangChain components in their app.


### How do I mask sensitive information in my runs?

If you need to completely hide the inputs and outputs of your traces, you can set the following environment variables when running your application:

```bash
LANGCHAIN_HIDE_INPUTS=true
LANGCHAIN_HIDE_OUTPUTS=true
```

This will block input or output data from being transmitted to the LangSmith API. The latency and sequence of operation types will still be visible, but all the
inputs and outputs will be unavailable. Token count tracking is not available in this setting.