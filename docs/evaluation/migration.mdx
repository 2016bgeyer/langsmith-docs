---
sidebar_label: Migrating to `evaluate`
sidebar_position: 7
---

# Migrating from `run_on_dataset` to `evaluate`

In python, we've introduced a cleaner `evaluate()` function to replace the `run_on_dataset` function. While we are not deprecating the `run_on_dataset` function, the new function lets you get started and without needing to install `langchain` in your local environment.

This guide will walk you through the process of migrating your existing code from using `run_on_dataset` to leveraging the benefits of `evaluate`.

## Key Differences

1. The "thing you are evaluating" (pipeline, target, model, chain, agent, etc.) is **always** the first positional argument and **always** has the following signature:

```python
def predict(inputs: dict) -> dict:
    """Call your model or pipeline with the given inputs and return the predictions."""
    # Example:
    # result = client.chat.completions.create(...)
    # response = result.choices[0].message.content
    return {"output": ...}
```

No need to specify the confusing "`llm_or_chain_factory`". If you need to create a new version of your object for each data point, initialize it within the `predict()` function.
If you want to evaluate a LangChain object (runnable, etc.), you can directly call `evaluate(chain.invoke, data: ...,...)`.

2. `dataset_name` is now `data` and accepts a broader range of inputs, including dataset names, ids, or an iterator over examples. This lets you easily evaluate over a subset of the data to quickly debug.

3. `RunEvalConfig` has been deprecated. Instead, directly provide a list of evaluators to the `evaluators` argument.

    a. Custom evaluators can simply be functions that take a `Run` and an `Example` and return a dictionary with the evaluation results. For example:

```python
def exact_match(run: Run, example: Example) -> dict:
    """Calculate the exact match score of the run."""
    expected = example.outputs["answer"]
    predicted = run.outputs["output"]
    return {"score": expected.lower() == predicted.lower(), "key": "exact_match"}

evaluate(
    ...,
    evaluators=[exact_match],

)
```
    
    b. LangChain evaluators can be incorporated using the `LangChainStringEvaluator` wrapper class.

For example, this evaluation:

```python
eval_config = RunEvalConfig(
    evaluators=[RunEvalConfig.Criteria(
        criteria={"usefulness": "The prediction is useful if..."},
        llm=my_eval_llm,
    )]
)

client.run_on_dataset(..., eval_config=eval_config)
```

Becomes:

```python
from langsmith.evaluation import LangChainStringEvaluator

evaluators=[
    LangChainStringEvaluator(
        "labeled_criteria",
        config={
            "criteria": {
                "usefulness": "The prediction is useful if...",
            }, 
            "llm": my_eval_llm,
        },
    ),
]
```


4. `batch_evaluators` are now `summary_evaluators`. These let you compute custom metrics over the whole dataset. For example, precision:

```python
def precision(runs: List[Run], examples: List[Example]) -> dict:
    """Calculate the precision of the runs."""
    expected = [example.outputs["answer"] for example in examples]
    predicted = [run.outputs["output"] for run in runs] 
    tp = sum([p == e for p, e in zip(predictions, expected) if p == "yes"])
    fp = sum([p == "yes" and e == "no" for p, e in zip(predictions, expected)])
    return {"score": tp / (tp + fp), "key": "precision"}
```

5. `project_metadata` is now simply `metadata`. 

6. `project_name` is replaced by `experiment_prefix`. `evaluate()` always appends an experiment uuid to the prefix to ensure uniqueness.

7. `concurrency_level` is now `max_concurrency`.

## Migration Steps

1. Update your imports:

```python
from langsmith.evaluation import evaluate
```

2. Change your `run_on_dataset` call to `evaluate`:

```python
results = evaluate(
    ...,
    data=...,
    evaluators=[...],
    summary_evaluators=[...],
    metadata=...,
    experiment_prefix=...,
    max_concurrency=...,
)
```

3. If you were using a factory function, replace it with a direct invocation:

```python
def predict(inputs: dict):
    my_pipeline = ...
    return my_pipeline.invoke(inputs)
```

4. Update your evaluators to use the new format:

```python
def accuracy(run: Run, example: Example):
    pred = run.outputs["output"]
    expected = example.outputs["answer"]
    return {"score": expected.lower() == pred.lower()}
```

5. If you were using LangChain evaluators, wrap them with `LangChainStringEvaluator`:

```python
from langsmith.evaluation import LangChainStringEvaluator

evaluators=[
    LangChainStringEvaluator("embedding_distance"),
    LangChainStringEvaluator(
        "labeled_criteria",
        config={"criteria": {"usefulness": "The prediction is useful if..."}},
        prepare_data=prepare_criteria_data
    ),
]
```

6. Update any references to `project_metadata`, `project_name`, and `concurrency_level` to use the new argument names.

7. If you were specifying a `dataset_version`, you can directly pass the dataset at the expected version like so:

```python
dataset_version = "lates" # your tagged version

results = evaluate(
    ...,
    data=client.list_examples(dataset_name="my_dataset", as_of=dataset_version),
    ...
)

```

## Support

If you encounter any issues during the migration process or have further questions, please don't hesitate to reach out to our support team at [support@langchain.dev](mailto:support@langchain.dev). We're here to help ensure a smooth transition!

Happy evaluating!