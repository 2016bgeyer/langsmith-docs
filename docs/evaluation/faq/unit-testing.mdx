---
sidebar_label: Unit Test 
sidebar_position: 3
---

# Unit Tests

Unit tests for LLMs are assertions and checks designed to **quickly** identify obvious bugs and regressions in your AI system. Relative to evaluations, tests are designed to be **fast** and **cheap** to run, focusing on **specific** functionality and edge cases. 
We recommend using LangSmith to track any unit tests that touch an LLM or other non-deterministic part of your AI system.

## Write `@unit` test

To instruct LangSmith to track a unit test, decorate your `pytest` test functions with `@unit`.
If you want to  track the full nested trace of the system being testsed, you can mark those functions with `@traceable`. For example:

```python
from langsmith import unit, traceable

@traceable  
def generate_sql(user_query):
    # Replace with your SQL generation logic
    # e.g., my_llm(my_prompt.format(user_query))
    return "SELECT * FROM customers"

@unit
def test_sql_generation_select_all():
    user_query = "Get all users from the customers table" 
    sql = generate_sql(user_query)
    assert sql == "SELECT * FROM customers"
```

## Run tests

To run your tests, use the `pytest` (or your preferred test provider) command in your terminal. For example:

```bash
pytest tests/
```

Each time you run this test suite, LangSmith will collect the test suite results as a new `TestSuiteResult`, logging the `pass` rate (1 for pass, 0 for fail) over all the applicable tests.

The test suite syncs to a corresponding dataset named after your package or github repository.

![Unit Test Example](../static/unit-test-suite.png)


## Going Further

`@unit` is designed to stay out of your way and works well with familiar `pytest` features. For example:

#### Dry-run mode

If you want to run the tests without syncing the results to LangSmith, you can set `LANGCHAIN_TEST_TRACKING=false` in your environment.

```bash
LANGCHAIN_TEST_TRACKING=false pytest tests/
```

The tests will run as normal, but the experiment logs will not be sent to LangSmith.

#### Defining inputs as fixtures

Pytest fixtures let you define functions that serve as reusable inputs for your tests. LangSmith automatically syncs any test case inputs defined as fixtures. For example:

```python
import pytest

@pytest.fixture
def user_query():
    return "Get all users from the customers table"

@pytest.fixture  
def expected_sql():
    return "SELECT * FROM customers"

# output_keys indicate which test arguments to save as 'outputs' in the dataset (Optional)
# Otherwise, all arguments are saved as 'inputs'  
@unit(output_keys=["expected_sql"])
def test_sql_generation_with_fixture(user_query, expected_sql):
    sql = generate_sql(user_query)
    assert sql == expected_sql
```

#### Parametrizing tests

Parametrizing tests lets you run the same assertions across multiple sets of inputs. Use `pytest`'s `parametrize` decorator to achieve this. For example:  

```python
@unit
@pytest.mark.parametrize(
    "user_query, expected_sql",
    [
        ("Get all users from the customers table", "SELECT * FROM customers"),
        ("Get all users from the orders table", "SELECT * FROM orders"),  
    ],
)
def test_sql_generation_parametrized(user_query, expected_sql):
    sql = generate_sql(user_query)
    assert sql == expected_sql  
```

**Note:** as the parametrized list grows, you may consider using `evaluate()` instead. This parallelizes the evaluation and makes it easier to control individual experiments and the corresponding dataset.

#### Caching 

LLMs on every commit in CI can get expensive. To save time and resources, LangSmith lets you cache results to disk. Any identical inputs will be loaded from the cache so you don't have to call out to your LLM provider unless there are changes to the model, prompt, or retrieved data.

To enable caching, run with `LANGCHAIN_TEST_CACHE=/my/cache/path`. For example:
  
```bash  
LANGCHAIN_TEST_CACHE=tests/cassettes pytest tests/my_llm_tests
```

All requests will be cached to `tests/cassettes` and loaded from there on subsequent runs. If you check this in to your repository, your CI will be able to use the cache as well.

#### Using `watch` mode

With caching enabled, you can iterate quickly on your tests using `watch` mode without worrying about unnecessarily hitting your LLM provider. For example, using [`pytest-watch`](https://pypi.org/project/pytest-watch/):

```bash
pip install pytest-watch
LANGCHAIN_TEST_CACHE=tests/cassettes ptw tests/my_llm_tests  
```

## Conclusion

Unit testing with LangSmith is a quick and effective way to catch bugs and regressions in your AI system. By instrumenting your test functions with `@unit`, you can automatically track test results as experiments, monitor performance on specific test cases, and catch issues before they make it to production. Tests are fast and cheap to write, making them an ideal starting point for teams that want to begin systematically testing their LLMs without a huge upfront investment. Try adding LangSmith to your test suite today and start gaining visibility into your model's behavior.

:::note
`@unit` is currently only supported in python version `>=0.1.42`. If you are interested in unit testing functionality in TypeScript or other languages, please let us know at [support@langchain.dev](mailto:support@langchain.dev).  
:::