---
sidebar_label: Synthetic Data for Evaluation 
sidebar_position: 8
---

# Synthetic Data for Evaluation

When prototyping a new chain or agent, you may not have enough real data to thoroughly evaluate how the component will behave. Simulating data and using LLMs to augment your dataset can help kickstart your development process. This guide will walk you through two techniques for generating synthetic data: paraphrasing existing examples and generating new inputs.

:::note Reliability
Synthetic data is not a full substitute for real data. The quality of the data generated by these methods depends on factors like the model, prompt, and existing data. Always inspect synthetic datasets to ensure they capture the information you want to model and align with your use case.
:::

### Prerequisites

This guide assumes you've already connected to LangSmith and have access to a dataset you want to expand.

## Using Paraphrasing

Paraphrasing existing inputs helps check if your chain's behavior is consistent across similar inputs. Since paraphrasing is a semantically invariant transformation, the outputs should remain the same as the original. Here's how to set it up:

### Step 1: Define the Paraphrase Generator

Create a chain for generating paraphrases using the `ChatOpenAI` model with custom system prompts.

```python
import re
from typing import List

from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI
from langchain.output_parsers import ListOutputParser
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)


class NumberedListOutputParser(ListOutputParser):
    def parse(self, output: str) -> List[str]:
        return re.findall(r"\d+\.\s+(.*?)\n", output)


paraphrase_llm = ChatOpenAI(temperature=0.5)
prompt_template = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template(
            "You are a helpful paraphrasing assistant tasked with rephrasing text."
        ),
        SystemMessagePromptTemplate.from_template("Input: <INPUT>{query}</INPUT>"),
        HumanMessagePromptTemplate.from_template(
            "What are {n_paraphrases} different ways you could paraphrase the INPUT text?"
            " Do not significantly change the meaning."
            " Respond using numbered bullets. If you cannot think of any,"
            " just say 'I don't know.'"
        ),
    ]
)

paraphrase_chain = LLMChain(
    llm=paraphrase_llm,
    prompt=prompt_template,
    output_parser=NumberedListOutputParser(),
)
```

### Step 2: Generate Paraphrases Concurrently

Write a function to generate paraphrases for each example in your dataset using asyncio for concurrency.

```python
import asyncio
import re
from typing import Any, Coroutine, Tuple

from langsmith.schemas import Example


async def gather_with_concurrency(n: int, *coros: Coroutine) -> list:
    """
    A helper function to run coroutines concurrently up to a specified limit.
    """
    semaphore = asyncio.Semaphore(n)

    async def sem_coro(coro: Coroutine) -> Any:
        async with semaphore:
            return await coro

    return await asyncio.gather(*(sem_coro(c) for c in coros))


async def generate_paraphrases(
    example: Example, n_paraphrases: int
) -> Tuple[Example, List[str]]:
    """
    A function to generate paraphrases for a given string.
    """
    # Assumes the input is a string. Additional preprocessing might be needed.
    example_input = next(iter(example.inputs.values()))
    result = await paraphrase_chain.arun(
        query=example_input, n_paraphrases=n_paraphrases
    )

    # Use regex to parse the result, getting just the text after the numbered bullets
    return example, result
```

### Step 3: Paraphrase the Dataset

Use the `Client` from LangSmith to access your dataset and generate paraphrases for it.

```python
from langsmith import Client


async def paraphrase_main(
    client: Client, n_paraphrases: int, dataset_name: str, concurrency: int = 20
) -> list:
    """
    The main function to generate paraphrases for a dataset.
    """
    dataset = client.read_dataset(dataset_name=dataset_name)
    examples = client.list_examples(dataset_id=dataset.id)
    coros = (generate_paraphrases(example, n_paraphrases) for example in examples)
    results = await gather_with_concurrency(concurrency, *coros)
    flattened_results = []
    for example, batch_r in results:
        input_key = next(iter(example.inputs))
        for r in batch_r:
            client.create_example(
                inputs={input_key: r},
                outputs=example.outputs,
                dataset_id=dataset.id,
            )
            flattened_results.append(r)
    return [r for res in results for r in res]
```

Run the main function to generate paraphrases:

```python
client = Client()
n_paraphrases = 3
dataset_name = "Your Dataset Name"
paraphrase_results = await paraphrase_main(client, n_paraphrases, dataset_name)
```

Replace "Your Dataset Name" with your actual dataset name. After running, your dataset should be roughly 3x the original size.

## Generating New Inputs

To expand the dataset's semantic range, we need to generate completely new and plausible inputs. This method examines a random set of 5 examples and creates 6 novel ones that align with the inferred system but are distinct enough to have likely originated from different individuals.

### Step 1: Define the New Input Generator

Create a chain for generating new inputs using the `ChatOpenAI` model with custom system prompts.

```python
input_gen_llm = ChatOpenAI(temperature=0.5)
input_gen_prompt_template = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template(
            "You are a creative assistant tasked with coming up with new inputs for an application."
        ),
        SystemMessagePromptTemplate.from_template("The following are some examples of inputs: \n{examples}"),
        HumanMessagePromptTemplate.from_template(
            "Can you generate {n_inputs} unique and plausible inputs that could be asked by different users?"
        ),
    ]
)

input_gen_chain = LLMChain(
    llm=input_gen_llm,
    prompt=input_gen_prompt_template,
    output_parser=NumberedListOutputParser(),
)
```

### Step 2: Generate New Inputs Concurrently

Write a function to generate new inputs based on a set of example inputs using asyncio for concurrency.

```python
async def generate_new_inputs(
    example_inputs: List[str], n_inputs: int
) -> List[str]:
    """
    A function to generate new inputs based on a list of example inputs.
    """
    example_inputs_str = '\n'.join(f"- {input}" for input in example_inputs)
    return await input_gen_chain.arun(
        examples=example_inputs_str, n_inputs=n_inputs
    )
```

### Step 3: Generate New Inputs for the Dataset

Use the Client from LangSmith to access your dataset and generate new inputs for it. Note that new inputs don't come with corresponding outputs, so you may need to manually label them or use a separate model to generate the outputs.

```python
import random


async def input_gen_main(
    client: Client, n_inputs: int, dataset_name: str, sample_size: int = 5
) -> list:
    """
    The main function to generate new inputs for a dataset.
    """
    dataset = client.read_dataset(dataset_name=dataset_name)
    examples = list(client.list_examples(dataset_id=dataset.id))
    example_inputs = [next(iter(example.inputs.values())) for example in random.sample(examples, sample_size)]
    new_inputs = await generate_new_inputs(example_inputs, n_inputs)
    for input in new_inputs:
        # As we don't have corresponding outputs for new inputs, we leave outputs as an empty dict.
        client.create_example(
            inputs={"input": input},
            outputs={},
            dataset_id=dataset.id,
        )
    return new_inputs
```

Run the main function to generate new inputs:

```python
n_inputs = 6
input_gen_results = await input_gen_main(client, n_inputs, dataset_name)
```

After running, your dataset should contain new examples that differ more significantly from the original ones.

### Considerations

Remember that the quality of the paraphrases and generated inputs will depend on the model and prompt used, and these approaches may not be appropriate for all use cases. Always check your augmented data to ensure it maintains the original meaning, aligns with the system's context, and is suitable for your application.

Synthetic data is most useful early in the development process, when you're trying to gauge how sensitive your chain or model is to input variations. By combining paraphrasing, new input generation, and other augmentation methods, you can expand and diversify your dataset to verify the feasibility and robustness of a feature before deploying it to production.