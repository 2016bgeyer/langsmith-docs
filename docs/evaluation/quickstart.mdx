---
sidebar_label: Quick Start
sidebar_position: 1
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import CodeBlock from "@theme/CodeBlock";
import {
  CodeTabs,
  PythonBlock,
  TypeScriptBlock,
} from "@site/src/components/InstructionsWithCode";
import { ClientInstallationCodeTabs } from "@site/src/components/ClientInstallation";

# Evaluation Quick Start

In this walkthrough, you will evaluate a chain over a dataset of examples. To do so, you will:

- Create a dataset of example inputs
- Define an LLM, chain, or agent to evaluate
- Configure and run the evaluation
- Review the resulting traces and evaluation feedback in LangSmith

## Prerequisites

This walkthrough assumes you have already installed LangChain and `openai` and configured your environment to connect to LangSmith.

```bash
pip install -U "langchain[openai]"

export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
export LANGCHAIN_API_KEY=<your api key>
```

## 1. Create a dataset

Upload a dataset to LangSmith to use for evaluation. For this example, we will upload a pre-made list of input examples.

For more information on other ways to create and use datasets, check out the [datasets](datasets) guide.

<CodeTabs
  tabs={[
    PythonBlock(`from langsmith import Client\n
example_inputs = [
  "a rap battle between Atticus Finch and Cicero",
  "a rap battle between Barbie and Oppenheimer",
  "a Pythonic rap battle between two swallows: one European and one African",
  "a rap battle between Aubrey Plaza and Stephen Colbert",
]\n
client = Client()
dataset_name = "Rap Battle Dataset"\n
# Storing inputs in a dataset lets us
# run chains and LLMs over a shared set of examples.
dataset = client.create_dataset(
    dataset_name=dataset_name, description="Rap battle prompts.",
)
client.create_examples(
    inputs=[{"question": q} for q in example_inputs],
    outputs=None,
    dataset_id=dataset.id,
)`),
  ]}
  groupId="client-language"
/>

## 2. Define cognitive architecture to evaluate

LangSmith can evaluate any [Runnable](https://python.langchain.com/docs/expression_language/interface) LangChain component or any custom function over this dataset.

If your cognitive architecture uses state, such as conversational memory, you can provide a constructor function that creates a new
instance of your object for each example row in the dataset. If your cognitive architecture is stateless, you can directly pass the object or function in.

Custom functions that are not LangChain components will be automatically wrapped in a [RunnableLambda](https://api.python.langchain.com/en/latest/schema/langchain.schema.runnable.base.RunnableLambda.html)
so that all inferences will be traced.

<CodeTabs
  tabs={[
    {
      value: "runnable",
      label: "Runnable",
      language: "python",
      content: `from langchain.schema.runnable import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI\n
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
prompt = ChatPromptTemplate.from_messages([("human", "Spit some bars about {input}.")])
chain = prompt | llm | StrOutputParser()`,
    },
    {
      value: "chain",
      label: "Chain or Agent",
      language: "python",
      content: `from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain\n
# Since chains and agents can be stateful (they can have memory),
# create a constructor to pass in to the run_on_dataset method.
def create_chain():
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    return LLMChain.from_string(llm, "Spit some bars about {input}.")
`,
    },
    {
      value: "llm",
      label: "LLM or Chat Model",
      language: "python",
      content: `from langchain_openai import ChatOpenAI\n
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
`,
    },
    {
      value: "custom-function",
      label: "Custom function",
      language: "python",
      content: `# You can also evaluate any arbitrary function over the dataset.
# The input to the function will be the inputs dictionary for each example.
def predict_result(input_: dict) -> dict:
    return {"output": "Bar Bar Bar"}`,
    },
    {
      value: "custom-class",
      label: "Custom class",
      language: "python",
      content: `# If your predictor is stateful (e.g. it has memory),
# You can create a new instance of the predictor for each row in the dataset.
class MyPredictor:
    def __init__(self):
        self.state = 0
    
    def predict(self, input_: dict) -> dict:
        if self.state > 0:
            raise ValueError("This predictor is stateful and can only be called once."")
        self.state += 1
        return {"output": f"Bar Bar Bar {self.state}"}\n
def create_object() -> MyPredictor:
    predictor = MyPredictor()
    # Return the function that will be called on the next row
    return predictor.predict\n
      `,
    },
  ]}
  groupId="evaluated-component"
/>

## 3. Evaluate

LangChain provides a convenient [run_on_dataset](https://api.python.langchain.com/en/latest/smith/langchain.smith.evaluation.runner_utils.run_on_dataset.html#langchain.smith.evaluation.runner_utils.run_on_dataset) and async [arun_on_dataset](https://api.python.langchain.com/en/latest/smith/langchain.smith.evaluation.runner_utils.arun_on_dataset.html#langchain.smith.evaluation.runner_utils.arun_on_dataset) method to generate predictions (and traces) over a dataset. When a [RunEvalConfig](https://api.python.langchain.com/en/latest/smith/langchain.smith.evaluation.config.RunEvalConfig.html#langchain.smith.evaluation.config.RunEvalConfig) is provided, the configured evalutors will be applied to the predictions as well to generate automated feedback.

Below, configure evaluation for some custom criteria. The feedback will be automatically logged within LangSmith. Since the input examples we created above lack "ground truth" reference labels, we will only select reference-free "Criteria" evaluators.

For more information on evaluators you can use off-the-shelf, check out the [pre-built evaluators](evaluator-implementations) docs or the [reference documentation](https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.evaluation) for LangChain's evalution module.
For more information on how to write a custom evaluator, check out the [custom evaluators](custom-evaluators) guide.

<CodeTabs
  tabs={[
    {
      value: "runnable",
      label: "Runnable",
      language: "python",
      content: `from langchain.smith import RunEvalConfig, run_on_dataset
from langsmith.evaluation import EvaluationResult, run_evaluator\n
@run_evaluator
def not_empty(run, example) -> EvaluationResult:
    score = bool(run.outputs is not None and run.outputs["output"].strip())
    return EvaluationResult(key="not_empty", score=score)\n\n
eval_config = RunEvalConfig(
    custom_evaluators=[not_empty],
    # You can also use a prebuilt evaluator
    # by providing a name or RunEvalConfig.<configured evaluator>
    evaluators=[
        # You can specify an evaluator by name/enum.
        # In this case, the default criterion is "helpfulness"
        "criteria",
        # Or you can configure the evaluator
        RunEvalConfig.Criteria("harmfulness"),
        RunEvalConfig.Criteria(
            {
                "cliche": "Are the lyrics cliche?"
                " Respond Y if they are, N if they're entirely unique."
            }
        ),
    ],
)
client.run_on_dataset(
    dataset_name=dataset_name,
    llm_or_chain_factory=chain,
    evaluation=eval_config,
    verbose=True,
    project_name="runnable-test-1",
)`,
    },
    {
      value: "chain",
      label: "Chain or Agent",
      language: "python",
      content: `from langchain.smith import RunEvalConfig, run_on_dataset
from langsmith.evaluation import EvaluationResult, run_evaluator\n
@run_evaluator
def not_empty(run, example) -> EvaluationResult:
    score = run.outputs is not None and run.outputs["text"].strip()
    return EvaluationResult(key="not_empty", score=score)\n
eval_config = RunEvalConfig(
    custom_evaluators=[not_empty],
    # You can also use a prebuilt evaluator
    # by providing a name or RunEvalConfig.<configured evaluator>
    evaluators=[
        # You can specify an evaluator by name/enum.
        # In this case, the default criterion is "helpfulness"
        "criteria",
        # Or you can configure the evaluator
        RunEvalConfig.Criteria("harmfulness"),
        RunEvalConfig.Criteria(
            {
                "cliche": "Are the lyrics cliche?"
                " Respond Y if they are, N if they're entirely unique."
            }
        ),
    ],
)
client.run_on_dataset(
    dataset_name=dataset_name,
    llm_or_chain_factory=create_chain,
    evaluation=eval_config,
    verbose=True,
    project_name="chain-test-1",
)`,
    },
    {
      value: "llm",
      label: "LLM or Chat Model",
      language: "python",
      content: `from langchain.smith import RunEvalConfig, run_on_dataset
from langsmith.evaluation import EvaluationResult, run_evaluator\n
@run_evaluator
def not_empty(run, example) -> EvaluationResult:
    score = run.outputs and run.outputs["generations"][0][0]["text"].strip()
    return EvaluationResult(key="not_empty", score=score)\n
eval_config = RunEvalConfig(
    custom_evaluators=[not_empty],
    # You can also use a prebuilt evaluator
    # by providing a name or RunEvalConfig.<configured evaluator>
    evaluators=[
        # You can specify an evaluator by name/enum.
        # In this case, the default criterion is "helpfulness"
        "criteria",
        # Or you can configure the evaluator
        RunEvalConfig.Criteria("harmfulness"),
        RunEvalConfig.Criteria(
            {
                "cliche": "Are the lyrics cliche?"
                " Respond Y if they are, N if they're entirely unique."
            }
        ),
    ],
)
client.run_on_dataset(
    dataset_name=dataset_name,
    llm_or_chain_factory=llm,
    evaluation=eval_config,
    verbose=True,
    project_name="chatopenai-test-1",
)`,
    },
    {
      value: "custom-function",
      label: "Custom function",
      language: "python",
      content: `from langchain.smith import RunEvalConfig, run_on_dataset
from langsmith.evaluation import EvaluationResult, run_evaluator\n
@run_evaluator
def not_empty(run, example) -> EvaluationResult:
    score = bool(run.outputs and run.outputs["output"])
    return EvaluationResult(key="not_empty", score=score)\n
eval_config = RunEvalConfig(
    custom_evaluators=[not_empty],
    # You can also use a prebuilt evaluator
    # by providing a name or RunEvalConfig.<configured evaluator>
    evaluators=[
        # You can specify an evaluator by name/enum.
        # In this case, the default criterion is "helpfulness"
        "criteria",
        # Or you can configure the evaluator
        RunEvalConfig.Criteria("harmfulness"),
        RunEvalConfig.Criteria(
            {
                "cliche": "Are the lyrics cliche?"
                " Respond Y if they are, N if they're entirely unique."
            }
        ),
    ],
)
client.run_on_dataset(
    dataset_name=dataset_name,
    llm_or_chain_factory=predict_result,
    evaluation=eval_config,
    verbose=True,
    project_name="custom-function-test-1",
)`,
    },
 {
      value: "custom-class",
      label: "Custom class",
      language: "python",
      content: `from langchain.smith import RunEvalConfig, run_on_dataset
from langsmith.evaluation import EvaluationResult, run_evaluator\n
@run_evaluator
def not_empty(run, example) -> EvaluationResult:
    score = run.outputs and run.outputs["output"]
    return EvaluationResult(key="not_empty", score=score)\n
eval_config = RunEvalConfig(
    custom_evaluators=[not_empty],
    # You can also use a prebuilt evaluator
    # by providing a name or RunEvalConfig.<configured evaluator>
    evaluators=[
        # You can specify an evaluator by name/enum.
        # In this case, the default criterion is "helpfulness"
        "criteria",
        # Or you can configure the evaluator
        RunEvalConfig.Criteria("harmfulness"),
        RunEvalConfig.Criteria(
            {
                "cliche": "Are the lyrics cliche?"
                " Respond Y if they are, N if they're entirely unique."
            }
        ),
    ],
)
client.run_on_dataset(
    dataset_name=dataset_name,
    # We are passing the "factory" function in this case.
    llm_or_chain_factory=create_object,
    evaluation=eval_config,
    verbose=True,
    project_name="custom-class-test-1",
)`,
    },
  ]}
  groupId="evaluated-component"
/>

## 4. Review Results

The evaluation results will be streamed to a new test project linked to your "Rap Battle Dataset". You can view the results by clicking on the link printed by the `run_on_dataset` function or by navigating to the [Datasets & Testing](https://smith.langchain.com/datasets) page, clicking "Rap Battle Dataset", and viewing the latest test run.

There, you can inspect the traces and feedback generated from the evaluation configuration.

![Eval test run screenshot](static/eval-test-run.png)

You can click on any row to view the trace and feedback generated for that example.

![Eval trace screenshot](static/eval-run-trace.png)

To view the outputs for other runs on that same example rows, click `View all reference runs`.

![Eval reference runs screenshot](static/eval-referenced-runs.png)

## More on evaluation

Congratulations! You've now created a dataset and used it to evaluate your agent or LLM.
To learn more about evaluation chains available out of the box, check out the [LangChain Evaluators](evaluator-implementations) guide. To learn how to make your own custom evaluators, review the [Custom Evaluator](custom-evaluators) guide.
