---
sidebar_label: Quick Start
sidebar_position: 1
---

# Quick Start

## What is the LangSmith Proxy

The LangSmith Proxy is intended to be a drop-in replacement for your LLM apis with some additional features. It is designed to be a simple, easy-to-use, and easy-to-configure tool that adds minimal overhead to your existing LLM API usage.

* **Easy to use**: The LangSmith LLM Proxy is designed to be easy to use. You can run it as a sidecar to your existing app and start using it immediately. Besides changing the URL of your LLM API, you don't need to make any changes to your app to start using the LangSmith Proxy.
* **Cache support**: The LangSmith LLM Proxy supports caching of requests/responses(with streaming support) from the LLM API. This allows you to cache responses for a configurable amount of time, reducing the number of requests to the LLM API and improving response times. This can be especially useful if you are using the LLM API in a high-traffic environment or running CI(in cases like evals)
* **Minimal overhead**: We use NGINX as a reverse proxy to minimize the overhead of the LangSmith LLM Proxy. Requests are passed through the proxy with minimal processing, and responses are passed back to the client with minimal additional processing.
* **Streaming support**: The LangSmith LLM Proxy supports streaming responses from the LLM API. This allows you to start processing the response as soon as it is available, rather than waiting for the entire response to be received.
* **Tracing support**: The LangSmith LLM Proxy supports tracing of LLM calls via LangSmith. This allows you to trace calls to your LLM without any configuration changes to your app.

## Models Supported

The LangSmith Proxy supports the following APIS:

* OpenAI(Chat and Completion)
* AzureOpenAI(Chat and Completion)

We are actively working on adding support for the following models:

* Anthropic
* Google Vertex
* Gemini

If you would like to see support for a specific model, please message us at support@langchain.dev


The steps in this guide will acquaint you with deploying the LangSmith Proxy and using it to make requests to OpenAI.

1. Launch the LangSmith Proxy container in your environment
2. Edit your app to make requests to the LangSmith Proxy
3. Forcefully turn off caching for the LangSmith Proxy
4. Cache a streamed request
5. Turn on tracing while using the LangSmith Proxy

## Prerequisites

1. Docker installed on your local machine
2. An OpenAI API Key
3. If Configuring tracing, a LangSmith API Key

## 1. Deploy the LangSmith Proxy

The LangSmith Proxy is available as a Docker container. You can run it in your environment by running the following command:

```bash
docker pull docker.io/langchain/langsmith-proxy:latest # Force pull the latest version of the LangSmith Proxy
docker run -p 8080:8080 docker.io/langchain/langsmith-proxy:latest -p 8080:8080 # Run the LangSmith Proxy on port 8080 and publish it to the host
```

You should see the following output:

```bash
2024-03-06 12:59:57,458 CRIT Supervisor is running as root.  Privileges were not dropped because no user is specified in the config file.  If you intend to run as root, you can set user=root in the config file to avoid this message.
2024-03-06 12:59:57,467 INFO supervisord started with pid 1
2024-03-06 12:59:58,503 INFO spawned: 'nginx' with pid 8
2024-03-06 12:59:58,552 INFO spawned: 'trace-processor' with pid 10
2024-03-06 12:59:59,562 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2024-03-06 12:59:59,563 INFO success: trace-processor entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
Couldn't create langsmith client: API key must be provided when using hosted LangSmith API, will skip creating runs
Listening for traces at 0.0.0.0:9999
Connection from ('127.0.0.1', 47370)
```
Ignore the `Couldn't create langsmith client` message if you are not configuring tracing.

## 2. Update your app to make requests to the LangSmith Proxy

For this example, we'll be using your local proxy running on `localhost:8080`. You can replace this with the address of your proxy if it's running on a different machine.

You may need to install some packages for this example:

```bash
pip install langchain-openai
pip install asyncio
```

Now, you can use the LangSmith Proxy to make requests to OpenAI. Here's an example of how you can do this in Python:
Let's create a file called `openai_test.py` and add the following code:
```python
import time

from langchain_openai import ChatOpenAI

# Different models can be accessed by changing the /proxy/openai to /proxy/<model>
OPENAI_API_URL = "http://localhost:8080/proxy/openai"

start = time.time()
llm = ChatOpenAI(openai_api_key="", openai_api_base=OPENAI_API_URL)
resp = llm.invoke("What is the meaning of life?")
print(resp)
print(f"Time taken: {time.time() - start}")
```

Run the file using the following command:
```bash
python openai_test.py

content='The meaning of life is a deeply philosophical and existential question that has been pondered by humans for centuries. There is no definitive answer to this question as it can vary greatly depending on individual beliefs, values, and perspectives. Some may find meaning in spiritual or religious beliefs, others may find it in personal relationships, achievements, or experiences. Ultimately, the meaning of life is subjective and can be different for everyone.'Time taken: 1.9526588916778564
Time taken: 1.9526588916778564
```

There will also be a corresponding Cache Miss log entry in your proxy.
```bash
"POST /proxy/openai/chat/completions HTTP/1.1" 200 20418 "-" "OpenAI/Python 1.12.0" "CACHE: MISS"
````

Pretty slow! Let's try running this again.

```bash
python openai_test.py

content='The meaning of life is a deeply philosophical question that has been pondered by humans for centuries. Different people and cultures have different beliefs about the purpose and meaning of life. Some believe that the meaning of life is to seek happiness, fulfillment, and personal growth, while others believe that it is to serve a higher power or contribute to the greater good of humanity. Ultimately, the meaning of life is a question that each individual must grapple with and find their own answer to.'
Time taken: 0.07127499580383301
```

Cache hit! Our request hits the cache and the response time is much faster. You will also see a Cache Hit log entry in your proxy.
```bash
"POST /proxy/openai/chat/completions HTTP/1.1" 200 24915 "-" "OpenAI/Python 1.12.0" "CACHE: HIT"
```

## 3. Forcefully turn off caching for the LangSmith Proxy
In some scenarios, you may want to forcefully turn off caching for the LangSmith Proxy. You can do this by adding a Cache-Control header `{"Cache-Control": "no-cache"}` header to your request. This will force the LangSmith Proxy to make a request to the LLM API and bypass the cache.

```python
import time

from langchain_openai import ChatOpenAI

# Different models can be accessed by changing the /proxy/openai to /proxy/<model>
OPENAI_API_URL = "http://localhost:8080/proxy/openai"

start = time.time()
llm_no_cache = ChatOpenAI(openai_api_key="", openai_api_base=OPENAI_API_URL, default_headers={"Cache-Control": "no-cache"})
resp = llm_no_cache.invoke("What is the meaning of life?")
print(resp)
print(f"Time taken: {time.time() - start}")
```

There will also be a corresponding Cache Bypass log entry in your proxy.
```bash
"POST /proxy/openai/chat/completions HTTP/1.1" 200 24376 "-" "OpenAI/Python 1.12.0" "CACHE: BYPASS"
````

As you can see, the request takes a longer time to complete as the LangSmith Proxy is making a request to the LLM API. You will also see a Bypass Header in your response.

## 4. Support for Streaming

The LangSmith Proxy supports streaming responses from the LLM API. This allows you to start processing the response as soon as it is available, rather than waiting for the entire response to be received.
This will still work with caching!

```python
import time

from langchain_openai import ChatOpenAI

# Different models can be accessed by changing the /proxy/openai to /proxy/<model>
OPENAI_API_URL = "http://localhost:8080/proxy/openai"

start = time.time()
llm = ChatOpenAI(openai_api_key="", openai_api_base=OPENAI_API_URL)
chunks = llm.stream("What is the meaning of life?")
for chunk in chunks:
    print(chunk)
print(f"Time taken: {time.time() - start}")
```

```bash
python openai_test.py

content=''
content='The'
content=' meaning'
content=' of'
content=' life'
content=' is'
content=' a'
content=' philosophical'
content=' question'
content=' that'
content=' has'
content=' been'
content=' debated'
content=' for'
content=' centuries'
content='.'
...
Time taken: 3.4340438842773438
```

Let's run this again and see how the response time changes.

```bash
content=''
content='The'
content=' meaning'
content=' of'
content=' life'
content=' is'
content=' a'
content=' philosophical'
content=' question'
content=' that'
content=' has'
content=' been'
content=' debated'
content=' for'
content=' centuries'
content='.'
content=' Different'
content=' people'
content=' and'
content=' cultures'
content=' have'
content=' different'
content=' beliefs'
content=' and'
content=' interpretations'
content=' of'
content=' the'
content=' meaning'
content=' of'
content=' life'
content='.'
...
Time taken: 0.07226419448852539
```

Again, much faster! The corresponding miss and hit logs will also be present in your proxy.
```
"POST /proxy/openai/chat/completions HTTP/1.1" 200 17761 "-" "OpenAI/Python 1.12.0" "CACHE: MISS"
"POST /proxy/openai/chat/completions HTTP/1.1" 200 17745 "-" "OpenAI/Python 1.12.0" "CACHE: HIT"
```


## 5. Turn on tracing while using the LangSmith Proxy

Requests made to the LangSmith proxy can also be traced.
You can do this by running the docker image with the `LANGSMITH_API_KEY` environment variable set to your LangSmith API Key.

```bash
docker run -p 8080:8080 -e LANGCHAIN_API_KEY=<your_langsmith_api_key> -e  docker.io/langchain/langsmith-proxy:latest -p 8080:8080
```

Run the script again and you should see a trace in your LangSmith dashboard.

```bash
python openai_test.py

# In nginx logs
Created run with inputs: {'messages': [{'role': 'user', 'content': 'What is the meaning of life?'}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stream': True, 'temperature': 0.7}
```

The corresponding trace also appears in your LangSmith dashboard!

[![LangSmith Trace](./static/trace.png)](./static/trace.png)

## Next Steps

- In this guide, you learned how to deploy the LangSmith Proxy and use it to proxy requests to OpenAI.
- We highly recommend trying this out with your own applications and models.
- Note that proxy is intended to run near your application to minimize latency. You can run it as a sidecar to your application or as a separate service.
- We also bundle this into the LangSmith Self-Hosted deployment by default. If you are using LangSmith Self-Hosted, you can use your LangSmith url as the proxy url.
